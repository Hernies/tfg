digraph {
	graph [size="161.1,161.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139917091875232 [label="
 (1, 23)" fillcolor=darkolivegreen1]
	139916966796112 [label=AddmmBackward0]
	139916966796160 -> 139916966796112
	139916967378752 [label="fc_class_count.bias
 (23)" fillcolor=lightblue]
	139916967378752 -> 139916966796160
	139916966796160 [label=AccumulateGrad]
	139916966796784 -> 139916966796112
	139916966796784 [label=ViewBackward0]
	139916966796496 -> 139916966796784
	139916966796496 [label=MeanBackward1]
	139916966795776 -> 139916966796496
	139916966795776 [label=LeakyReluBackward1]
	139916966795632 -> 139916966795776
	139916966795632 [label=NativeBatchNormBackward0]
	139916966795584 -> 139916966795632
	139916966795584 [label=ConvolutionBackward0]
	139916966795200 -> 139916966795584
	139916966795200 [label=CatBackward0]
	139916966794864 -> 139916966795200
	139916966794864 [label=SplitBackward0]
	139916966794672 -> 139916966794864
	139916966794672 [label=NativeBatchNormBackward0]
	139916966794624 -> 139916966794672
	139916966794624 [label=ConvolutionBackward0]
	139916966794240 -> 139916966794624
	139916966794240 [label=LeakyReluBackward1]
	139916966793904 -> 139916966794240
	139916966793904 [label=NativeBatchNormBackward0]
	139916966793808 -> 139916966793904
	139916966793808 [label=ConvolutionBackward0]
	139916966793424 -> 139916966793808
	139916966793424 [label=LeakyReluBackward1]
	139916966793280 -> 139916966793424
	139916966793280 [label=NativeBatchNormBackward0]
	139916966764352 -> 139916966793280
	139916966764352 [label=ConvolutionBackward0]
	139916966764016 -> 139916966764352
	139916966764016 [label=CatBackward0]
	139916966763728 -> 139916966764016
	139916966763728 [label=SplitBackward0]
	139916966763584 -> 139916966763728
	139916966763584 [label=NativeBatchNormBackward0]
	139916966763392 -> 139916966763584
	139916966763392 [label=ConvolutionBackward0]
	139916966763056 -> 139916966763392
	139916966763056 [label=LeakyReluBackward1]
	139916966762768 -> 139916966763056
	139916966762768 [label=NativeBatchNormBackward0]
	139916966762576 -> 139916966762768
	139916966762576 [label=ConvolutionBackward0]
	139916966762288 -> 139916966762576
	139916966762288 [label=LeakyReluBackward1]
	139916966762096 -> 139916966762288
	139916966762096 [label=NativeBatchNormBackward0]
	139916966762048 -> 139916966762096
	139916966762048 [label=ConvolutionBackward0]
	139916966761664 -> 139916966762048
	139916966761664 [label=CatBackward0]
	139916966761328 -> 139916966761664
	139916966761328 [label=SplitBackward0]
	139916966761136 -> 139916966761328
	139916966761136 [label=NativeBatchNormBackward0]
	139916966761088 -> 139916966761136
	139916966761088 [label=ConvolutionBackward0]
	139916966760704 -> 139916966761088
	139916966760704 [label=LeakyReluBackward1]
	139916966760512 -> 139916966760704
	139916966760512 [label=NativeBatchNormBackward0]
	139916966731536 -> 139916966760512
	139916966731536 [label=ConvolutionBackward0]
	139916966731152 -> 139916966731536
	139916966731152 [label=LeakyReluBackward1]
	139916966731008 -> 139916966731152
	139916966731008 [label=NativeBatchNormBackward0]
	139916966730816 -> 139916966731008
	139916966730816 [label=ConvolutionBackward0]
	139916966730480 -> 139916966730816
	139916966730480 [label=CatBackward0]
	139916966730192 -> 139916966730480
	139916966730192 [label=SplitBackward0]
	139916966730048 -> 139916966730192
	139916966730048 [label=NativeBatchNormBackward0]
	139916966729856 -> 139916966730048
	139916966729856 [label=ConvolutionBackward0]
	139916966729520 -> 139916966729856
	139916966729520 [label=MaxPool2DWithIndicesBackward0]
	139916966729232 -> 139916966729520
	139916966729232 [label=LeakyReluBackward1]
	139916966729040 -> 139916966729232
	139916966729040 [label=NativeBatchNormBackward0]
	139916966728848 -> 139916966729040
	139916966728848 [label=ConvolutionBackward0]
	139916966728656 -> 139916966728848
	139917011348912 [label="backbone.stem.conv1.conv.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139917011348912 -> 139916966728656
	139916966728656 [label=AccumulateGrad]
	139916966729088 -> 139916966729040
	139917011348832 [label="backbone.stem.conv1.bn.weight
 (64)" fillcolor=lightblue]
	139917011348832 -> 139916966729088
	139916966729088 [label=AccumulateGrad]
	139916966729424 -> 139916966729040
	139917011348752 [label="backbone.stem.conv1.bn.bias
 (64)" fillcolor=lightblue]
	139917011348752 -> 139916966729424
	139916966729424 [label=AccumulateGrad]
	139916966729616 -> 139916966729856
	139917011347312 [label="backbone.stages.0.conv_exp.conv.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139917011347312 -> 139916966729616
	139916966729616 [label=AccumulateGrad]
	139916966729808 -> 139916966730048
	139917011347392 [label="backbone.stages.0.conv_exp.bn.weight
 (256)" fillcolor=lightblue]
	139917011347392 -> 139916966729808
	139916966729808 [label=AccumulateGrad]
	139916966730240 -> 139916966730048
	139917011347232 [label="backbone.stages.0.conv_exp.bn.bias
 (256)" fillcolor=lightblue]
	139917011347232 -> 139916966730240
	139916966730240 [label=AccumulateGrad]
	139916966730432 -> 139916966730480
	139916966730432 [label=LeakyReluBackward1]
	139916966729664 -> 139916966730432
	139916966729664 [label=NativeBatchNormBackward0]
	139916966729280 -> 139916966729664
	139916966729280 [label=ConvolutionBackward0]
	139916966728560 -> 139916966729280
	139916966728560 [label=LeakyReluBackward0]
	139916966728272 -> 139916966728560
	139916966728272 [label=AddBackward0]
	139916966728080 -> 139916966728272
	139916966728080 [label=NativeBatchNormBackward0]
	139916966727936 -> 139916966728080
	139916966727936 [label=ConvolutionBackward0]
	139916966727744 -> 139916966727936
	139916966727744 [label=LeakyReluBackward1]
	139916966702672 -> 139916966727744
	139916966702672 [label=NativeBatchNormBackward0]
	139916966702480 -> 139916966702672
	139916966702480 [label=ConvolutionBackward0]
	139916966702192 -> 139916966702480
	139916966702192 [label=LeakyReluBackward1]
	139916966702000 -> 139916966702192
	139916966702000 [label=NativeBatchNormBackward0]
	139916966701952 -> 139916966702000
	139916966701952 [label=ConvolutionBackward0]
	139916966728320 -> 139916966701952
	139916966728320 [label=LeakyReluBackward0]
	139916966701376 -> 139916966728320
	139916966701376 [label=AddBackward0]
	139916966701184 -> 139916966701376
	139916966701184 [label=NativeBatchNormBackward0]
	139916966700944 -> 139916966701184
	139916966700944 [label=ConvolutionBackward0]
	139916966700560 -> 139916966700944
	139916966700560 [label=LeakyReluBackward1]
	139916966700416 -> 139916966700560
	139916966700416 [label=NativeBatchNormBackward0]
	139916966700224 -> 139916966700416
	139916966700224 [label=ConvolutionBackward0]
	139916966700032 -> 139916966700224
	139916966700032 [label=LeakyReluBackward1]
	139916966699600 -> 139916966700032
	139916966699600 [label=NativeBatchNormBackward0]
	139916966699408 -> 139916966699600
	139916966699408 [label=ConvolutionBackward0]
	139916966701232 -> 139916966699408
	139916966701232 [label=LeakyReluBackward0]
	139916966699072 -> 139916966701232
	139916966699072 [label=AddBackward0]
	139916966699312 -> 139916966699072
	139916966699312 [label=NativeBatchNormBackward0]
	139916966674000 -> 139916966699312
	139916966674000 [label=ConvolutionBackward0]
	139916966673712 -> 139916966674000
	139916966673712 [label=LeakyReluBackward1]
	139916966673520 -> 139916966673712
	139916966673520 [label=NativeBatchNormBackward0]
	139916966673472 -> 139916966673520
	139916966673472 [label=ConvolutionBackward0]
	139916966673088 -> 139916966673472
	139916966673088 [label=LeakyReluBackward1]
	139916966672752 -> 139916966673088
	139916966672752 [label=NativeBatchNormBackward0]
	139916966672656 -> 139916966672752
	139916966672656 [label=ConvolutionBackward0]
	139916966730192 -> 139916966672656
	139916966672272 -> 139916966672656
	139917011345632 [label="backbone.stages.0.blocks.0.conv1.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139917011345632 -> 139916966672272
	139916966672272 [label=AccumulateGrad]
	139916966672704 -> 139916966672752
	139917099063552 [label="backbone.stages.0.blocks.0.conv1.bn.weight
 (128)" fillcolor=lightblue]
	139917099063552 -> 139916966672704
	139916966672704 [label=AccumulateGrad]
	139916966672848 -> 139916966672752
	139917010835232 [label="backbone.stages.0.blocks.0.conv1.bn.bias
 (128)" fillcolor=lightblue]
	139917010835232 -> 139916966672848
	139916966672848 [label=AccumulateGrad]
	139916966673040 -> 139916966673472
	139917010834752 [label="backbone.stages.0.blocks.0.conv2.conv.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	139917010834752 -> 139916966673040
	139916966673040 [label=AccumulateGrad]
	139916966673424 -> 139916966673520
	139917010834912 [label="backbone.stages.0.blocks.0.conv2.bn.weight
 (128)" fillcolor=lightblue]
	139917010834912 -> 139916966673424
	139916966673424 [label=AccumulateGrad]
	139916966673664 -> 139916966673520
	139917010835072 [label="backbone.stages.0.blocks.0.conv2.bn.bias
 (128)" fillcolor=lightblue]
	139917010835072 -> 139916966673664
	139916966673664 [label=AccumulateGrad]
	139916966673856 -> 139916966674000
	139917010834272 [label="backbone.stages.0.blocks.0.conv3.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139917010834272 -> 139916966673856
	139916966673856 [label=AccumulateGrad]
	139916966674240 -> 139916966699312
	139917010834432 [label="backbone.stages.0.blocks.0.conv3.bn.weight
 (128)" fillcolor=lightblue]
	139917010834432 -> 139916966674240
	139916966674240 [label=AccumulateGrad]
	139916966674384 -> 139916966699312
	139917010834592 [label="backbone.stages.0.blocks.0.conv3.bn.bias
 (128)" fillcolor=lightblue]
	139917010834592 -> 139916966674384
	139916966674384 [label=AccumulateGrad]
	139916966730192 -> 139916966699072
	139916966699216 -> 139916966699408
	139917010833952 [label="backbone.stages.0.blocks.1.conv1.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139917010833952 -> 139916966699216
	139916966699216 [label=AccumulateGrad]
	139916966699648 -> 139916966699600
	139917010834112 [label="backbone.stages.0.blocks.1.conv1.bn.weight
 (128)" fillcolor=lightblue]
	139917010834112 -> 139916966699648
	139916966699648 [label=AccumulateGrad]
	139916966699792 -> 139916966699600
	139917010833552 [label="backbone.stages.0.blocks.1.conv1.bn.bias
 (128)" fillcolor=lightblue]
	139917010833552 -> 139916966699792
	139916966699792 [label=AccumulateGrad]
	139916966699984 -> 139916966700224
	139917010835872 [label="backbone.stages.0.blocks.1.conv2.conv.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	139917010835872 -> 139916966699984
	139916966699984 [label=AccumulateGrad]
	139916966700272 -> 139916966700416
	139917010835952 [label="backbone.stages.0.blocks.1.conv2.bn.weight
 (128)" fillcolor=lightblue]
	139917010835952 -> 139916966700272
	139916966700272 [label=AccumulateGrad]
	139916966700608 -> 139916966700416
	139917010835712 [label="backbone.stages.0.blocks.1.conv2.bn.bias
 (128)" fillcolor=lightblue]
	139917010835712 -> 139916966700608
	139916966700608 [label=AccumulateGrad]
	139916966700800 -> 139916966700944
	139917010836192 [label="backbone.stages.0.blocks.1.conv3.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139917010836192 -> 139916966700800
	139916966700800 [label=AccumulateGrad]
	139916966701040 -> 139916966701184
	139917010836272 [label="backbone.stages.0.blocks.1.conv3.bn.weight
 (128)" fillcolor=lightblue]
	139917010836272 -> 139916966701040
	139916966701040 [label=AccumulateGrad]
	139916966701136 -> 139916966701184
	139917010836352 [label="backbone.stages.0.blocks.1.conv3.bn.bias
 (128)" fillcolor=lightblue]
	139917010836352 -> 139916966701136
	139916966701136 [label=AccumulateGrad]
	139916966701232 -> 139916966701376
	139916966701568 -> 139916966701952
	139917010836752 [label="backbone.stages.0.blocks.2.conv1.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139917010836752 -> 139916966701568
	139916966701568 [label=AccumulateGrad]
	139916966701904 -> 139916966702000
	139917010836832 [label="backbone.stages.0.blocks.2.conv1.bn.weight
 (128)" fillcolor=lightblue]
	139917010836832 -> 139916966701904
	139916966701904 [label=AccumulateGrad]
	139916966702144 -> 139916966702000
	139917010836912 [label="backbone.stages.0.blocks.2.conv1.bn.bias
 (128)" fillcolor=lightblue]
	139917010836912 -> 139916966702144
	139916966702144 [label=AccumulateGrad]
	139916966702336 -> 139916966702480
	139917010837392 [label="backbone.stages.0.blocks.2.conv2.conv.weight
 (128, 4, 3, 3)" fillcolor=lightblue]
	139917010837392 -> 139916966702336
	139916966702336 [label=AccumulateGrad]
	139916966702720 -> 139916966702672
	139916968591424 [label="backbone.stages.0.blocks.2.conv2.bn.weight
 (128)" fillcolor=lightblue]
	139916968591424 -> 139916966702720
	139916966702720 [label=AccumulateGrad]
	139916966702864 -> 139916966702672
	139916968591504 [label="backbone.stages.0.blocks.2.conv2.bn.bias
 (128)" fillcolor=lightblue]
	139916968591504 -> 139916966702864
	139916966702864 [label=AccumulateGrad]
	139916966702960 -> 139916966727936
	139916968591824 [label="backbone.stages.0.blocks.2.conv3.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139916968591824 -> 139916966702960
	139916966702960 [label=AccumulateGrad]
	139916966727888 -> 139916966728080
	139916968591904 [label="backbone.stages.0.blocks.2.conv3.bn.weight
 (128)" fillcolor=lightblue]
	139916968591904 -> 139916966727888
	139916966727888 [label=AccumulateGrad]
	139916966728128 -> 139916966728080
	139916968591984 [label="backbone.stages.0.blocks.2.conv3.bn.bias
 (128)" fillcolor=lightblue]
	139916968591984 -> 139916966728128
	139916966728128 [label=AccumulateGrad]
	139916966728320 -> 139916966728272
	139916966728464 -> 139916966729280
	139916968592304 [label="backbone.stages.0.conv_transition_b.conv.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	139916968592304 -> 139916966728464
	139916966728464 [label=AccumulateGrad]
	139916966728896 -> 139916966729664
	139916968592384 [label="backbone.stages.0.conv_transition_b.bn.weight
 (128)" fillcolor=lightblue]
	139916968592384 -> 139916966728896
	139916966728896 [label=AccumulateGrad]
	139916966730000 -> 139916966729664
	139916968592464 [label="backbone.stages.0.conv_transition_b.bn.bias
 (128)" fillcolor=lightblue]
	139916968592464 -> 139916966730000
	139916966730000 [label=AccumulateGrad]
	139916966730576 -> 139916966730816
	139916968592864 [label="backbone.stages.0.conv_transition.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968592864 -> 139916966730576
	139916966730576 [label=AccumulateGrad]
	139916966730768 -> 139916966731008
	139916968592944 [label="backbone.stages.0.conv_transition.bn.weight
 (256)" fillcolor=lightblue]
	139916968592944 -> 139916966730768
	139916966730768 [label=AccumulateGrad]
	139916966731200 -> 139916966731008
	139916968593024 [label="backbone.stages.0.conv_transition.bn.bias
 (256)" fillcolor=lightblue]
	139916968593024 -> 139916966731200
	139916966731200 [label=AccumulateGrad]
	139916966731392 -> 139916966731536
	139916968593504 [label="backbone.stages.1.conv_down.conv.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	139916968593504 -> 139916966731392
	139916966731392 [label=AccumulateGrad]
	139916966731584 -> 139916966760512
	139916968593584 [label="backbone.stages.1.conv_down.bn.weight
 (256)" fillcolor=lightblue]
	139916968593584 -> 139916966731584
	139916966731584 [label=AccumulateGrad]
	139916966731728 -> 139916966760512
	139916968593664 [label="backbone.stages.1.conv_down.bn.bias
 (256)" fillcolor=lightblue]
	139916968593664 -> 139916966731728
	139916966731728 [label=AccumulateGrad]
	139916966760656 -> 139916966761088
	139916968593984 [label="backbone.stages.1.conv_exp.conv.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139916968593984 -> 139916966760656
	139916966760656 [label=AccumulateGrad]
	139916966761040 -> 139916966761136
	139916968594064 [label="backbone.stages.1.conv_exp.bn.weight
 (512)" fillcolor=lightblue]
	139916968594064 -> 139916966761040
	139916966761040 [label=AccumulateGrad]
	139916966761280 -> 139916966761136
	139916968594144 [label="backbone.stages.1.conv_exp.bn.bias
 (512)" fillcolor=lightblue]
	139916968594144 -> 139916966761280
	139916966761280 [label=AccumulateGrad]
	139916966761472 -> 139916966761664
	139916966761472 [label=LeakyReluBackward1]
	139916966760896 -> 139916966761472
	139916966760896 [label=NativeBatchNormBackward0]
	139916966761232 -> 139916966760896
	139916966761232 [label=ConvolutionBackward0]
	139916966730960 -> 139916966761232
	139916966730960 [label=LeakyReluBackward0]
	139916966730384 -> 139916966730960
	139916966730384 [label=AddBackward0]
	139916966727792 -> 139916966730384
	139916966727792 [label=NativeBatchNormBackward0]
	139916966728512 -> 139916966727792
	139916966728512 [label=ConvolutionBackward0]
	139916966701520 -> 139916966728512
	139916966701520 [label=LeakyReluBackward1]
	139916966700992 -> 139916966701520
	139916966700992 [label=NativeBatchNormBackward0]
	139916966701328 -> 139916966700992
	139916966701328 [label=ConvolutionBackward0]
	139916966700368 -> 139916966701328
	139916966700368 [label=LeakyReluBackward1]
	139916966699840 -> 139916966700368
	139916966699840 [label=NativeBatchNormBackward0]
	139916966699120 -> 139916966699840
	139916966699120 [label=ConvolutionBackward0]
	139916966728704 -> 139916966699120
	139916966728704 [label=LeakyReluBackward0]
	139916966672512 -> 139916966728704
	139916966672512 [label=AddBackward0]
	139916966672896 -> 139916966672512
	139916966672896 [label=NativeBatchNormBackward0]
	139916966672464 -> 139916966672896
	139916966672464 [label=ConvolutionBackward0]
	139916966671792 -> 139916966672464
	139916966671792 [label=LeakyReluBackward1]
	139916966671600 -> 139916966671792
	139916966671600 [label=NativeBatchNormBackward0]
	139916966671552 -> 139916966671600
	139916966671552 [label=ConvolutionBackward0]
	139916966671168 -> 139916966671552
	139916966671168 [label=LeakyReluBackward1]
	139916966670832 -> 139916966671168
	139916966670832 [label=NativeBatchNormBackward0]
	139916966670736 -> 139916966670832
	139916966670736 [label=ConvolutionBackward0]
	139916966672560 -> 139916966670736
	139916966672560 [label=LeakyReluBackward0]
	139916966670544 -> 139916966672560
	139916966670544 [label=AddBackward0]
	139916966645328 -> 139916966670544
	139916966645328 [label=NativeBatchNormBackward0]
	139916966645232 -> 139916966645328
	139916966645232 [label=ConvolutionBackward0]
	139916966644944 -> 139916966645232
	139916966644944 [label=LeakyReluBackward1]
	139916966644800 -> 139916966644944
	139916966644800 [label=NativeBatchNormBackward0]
	139916966644608 -> 139916966644800
	139916966644608 [label=ConvolutionBackward0]
	139916966644224 -> 139916966644608
	139916966644224 [label=LeakyReluBackward1]
	139916966643984 -> 139916966644224
	139916966643984 [label=NativeBatchNormBackward0]
	139916966643792 -> 139916966643984
	139916966643792 [label=ConvolutionBackward0]
	139916966761328 -> 139916966643792
	139916966643408 -> 139916966643792
	139916968594624 [label="backbone.stages.1.blocks.0.conv1.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968594624 -> 139916966643408
	139916966643408 [label=AccumulateGrad]
	139916966644032 -> 139916966643984
	139916968594704 [label="backbone.stages.1.blocks.0.conv1.bn.weight
 (256)" fillcolor=lightblue]
	139916968594704 -> 139916966644032
	139916966644032 [label=AccumulateGrad]
	139916966644176 -> 139916966643984
	139916968594784 [label="backbone.stages.1.blocks.0.conv1.bn.bias
 (256)" fillcolor=lightblue]
	139916968594784 -> 139916966644176
	139916966644176 [label=AccumulateGrad]
	139916966644272 -> 139916966644608
	139916968595264 [label="backbone.stages.1.blocks.0.conv2.conv.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	139916968595264 -> 139916966644272
	139916966644272 [label=AccumulateGrad]
	139916966644560 -> 139916966644800
	139916968595344 [label="backbone.stages.1.blocks.0.conv2.bn.weight
 (256)" fillcolor=lightblue]
	139916968595344 -> 139916966644560
	139916966644560 [label=AccumulateGrad]
	139916966644992 -> 139916966644800
	139916968804416 [label="backbone.stages.1.blocks.0.conv2.bn.bias
 (256)" fillcolor=lightblue]
	139916968804416 -> 139916966644992
	139916966644992 [label=AccumulateGrad]
	139916966645040 -> 139916966645232
	139916968804736 [label="backbone.stages.1.blocks.0.conv3.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968804736 -> 139916966645040
	139916966645040 [label=AccumulateGrad]
	139916966645376 -> 139916966645328
	139916968804816 [label="backbone.stages.1.blocks.0.conv3.bn.weight
 (256)" fillcolor=lightblue]
	139916968804816 -> 139916966645376
	139916966645376 [label=AccumulateGrad]
	139916966645712 -> 139916966645328
	139916968804896 [label="backbone.stages.1.blocks.0.conv3.bn.bias
 (256)" fillcolor=lightblue]
	139916968804896 -> 139916966645712
	139916966645712 [label=AccumulateGrad]
	139916966761328 -> 139916966670544
	139916966670400 -> 139916966670736
	139916968805296 [label="backbone.stages.1.blocks.1.conv1.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968805296 -> 139916966670400
	139916966670400 [label=AccumulateGrad]
	139916966670784 -> 139916966670832
	139916968805376 [label="backbone.stages.1.blocks.1.conv1.bn.weight
 (256)" fillcolor=lightblue]
	139916968805376 -> 139916966670784
	139916966670784 [label=AccumulateGrad]
	139916966670928 -> 139916966670832
	139916968805456 [label="backbone.stages.1.blocks.1.conv1.bn.bias
 (256)" fillcolor=lightblue]
	139916968805456 -> 139916966670928
	139916966670928 [label=AccumulateGrad]
	139916966671120 -> 139916966671552
	139916968805936 [label="backbone.stages.1.blocks.1.conv2.conv.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	139916968805936 -> 139916966671120
	139916966671120 [label=AccumulateGrad]
	139916966671504 -> 139916966671600
	139916968806016 [label="backbone.stages.1.blocks.1.conv2.bn.weight
 (256)" fillcolor=lightblue]
	139916968806016 -> 139916966671504
	139916966671504 [label=AccumulateGrad]
	139916966671744 -> 139916966671600
	139916968806096 [label="backbone.stages.1.blocks.1.conv2.bn.bias
 (256)" fillcolor=lightblue]
	139916968806096 -> 139916966671744
	139916966671744 [label=AccumulateGrad]
	139916966671936 -> 139916966672464
	139916968806416 [label="backbone.stages.1.blocks.1.conv3.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968806416 -> 139916966671936
	139916966671936 [label=AccumulateGrad]
	139916966672320 -> 139916966672896
	139916968806496 [label="backbone.stages.1.blocks.1.conv3.bn.weight
 (256)" fillcolor=lightblue]
	139916968806496 -> 139916966672320
	139916966672320 [label=AccumulateGrad]
	139916966672080 -> 139916966672896
	139916968806576 [label="backbone.stages.1.blocks.1.conv3.bn.bias
 (256)" fillcolor=lightblue]
	139916968806576 -> 139916966672080
	139916966672080 [label=AccumulateGrad]
	139916966672560 -> 139916966672512
	139916966673232 -> 139916966699120
	139916968806976 [label="backbone.stages.1.blocks.2.conv1.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968806976 -> 139916966673232
	139916966673232 [label=AccumulateGrad]
	139916966699264 -> 139916966699840
	139916968807056 [label="backbone.stages.1.blocks.2.conv1.bn.weight
 (256)" fillcolor=lightblue]
	139916968807056 -> 139916966699264
	139916966699264 [label=AccumulateGrad]
	139916966674048 -> 139916966699840
	139916968807136 [label="backbone.stages.1.blocks.2.conv1.bn.bias
 (256)" fillcolor=lightblue]
	139916968807136 -> 139916966674048
	139916966674048 [label=AccumulateGrad]
	139916966700176 -> 139916966701328
	139916968807616 [label="backbone.stages.1.blocks.2.conv2.conv.weight
 (256, 8, 3, 3)" fillcolor=lightblue]
	139916968807616 -> 139916966700176
	139916966700176 [label=AccumulateGrad]
	139916966701760 -> 139916966700992
	139916968807696 [label="backbone.stages.1.blocks.2.conv2.bn.weight
 (256)" fillcolor=lightblue]
	139916968807696 -> 139916966701760
	139916966701760 [label=AccumulateGrad]
	139916966701712 -> 139916966700992
	139916968807776 [label="backbone.stages.1.blocks.2.conv2.bn.bias
 (256)" fillcolor=lightblue]
	139916968807776 -> 139916966701712
	139916966701712 [label=AccumulateGrad]
	139916966702912 -> 139916966728512
	139916968808096 [label="backbone.stages.1.blocks.2.conv3.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968808096 -> 139916966702912
	139916966702912 [label=AccumulateGrad]
	139916966728752 -> 139916966727792
	139916968808176 [label="backbone.stages.1.blocks.2.conv3.bn.weight
 (256)" fillcolor=lightblue]
	139916968808176 -> 139916966728752
	139916966728752 [label=AccumulateGrad]
	139916966703056 -> 139916966727792
	139916968808256 [label="backbone.stages.1.blocks.2.conv3.bn.bias
 (256)" fillcolor=lightblue]
	139916968808256 -> 139916966703056
	139916966703056 [label=AccumulateGrad]
	139916966728704 -> 139916966730384
	139916966730672 -> 139916966761232
	139916968517856 [label="backbone.stages.1.conv_transition_b.conv.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	139916968517856 -> 139916966730672
	139916966730672 [label=AccumulateGrad]
	139916966731440 -> 139916966760896
	139916968517936 [label="backbone.stages.1.conv_transition_b.bn.weight
 (256)" fillcolor=lightblue]
	139916968517936 -> 139916966731440
	139916966731440 [label=AccumulateGrad]
	139916966731344 -> 139916966760896
	139916968518016 [label="backbone.stages.1.conv_transition_b.bn.bias
 (256)" fillcolor=lightblue]
	139916968518016 -> 139916966731344
	139916966731344 [label=AccumulateGrad]
	139916966761616 -> 139916966762048
	139916968518416 [label="backbone.stages.1.conv_transition.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968518416 -> 139916966761616
	139916966761616 [label=AccumulateGrad]
	139916966762000 -> 139916966762096
	139916968518496 [label="backbone.stages.1.conv_transition.bn.weight
 (512)" fillcolor=lightblue]
	139916968518496 -> 139916966762000
	139916966762000 [label=AccumulateGrad]
	139916966762240 -> 139916966762096
	139916968518576 [label="backbone.stages.1.conv_transition.bn.bias
 (512)" fillcolor=lightblue]
	139916968518576 -> 139916966762240
	139916966762240 [label=AccumulateGrad]
	139916966762432 -> 139916966762576
	139916968519056 [label="backbone.stages.2.conv_down.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916968519056 -> 139916966762432
	139916966762432 [label=AccumulateGrad]
	139916966762816 -> 139916966762768
	139916968519136 [label="backbone.stages.2.conv_down.bn.weight
 (512)" fillcolor=lightblue]
	139916968519136 -> 139916966762816
	139916966762816 [label=AccumulateGrad]
	139916966762960 -> 139916966762768
	139916968519216 [label="backbone.stages.2.conv_down.bn.bias
 (512)" fillcolor=lightblue]
	139916968519216 -> 139916966762960
	139916966762960 [label=AccumulateGrad]
	139916966763152 -> 139916966763392
	139916968519536 [label="backbone.stages.2.conv_exp.conv.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139916968519536 -> 139916966763152
	139916966763152 [label=AccumulateGrad]
	139916966763344 -> 139916966763584
	139916968519616 [label="backbone.stages.2.conv_exp.bn.weight
 (1024)" fillcolor=lightblue]
	139916968519616 -> 139916966763344
	139916966763344 [label=AccumulateGrad]
	139916966763776 -> 139916966763584
	139916968519696 [label="backbone.stages.2.conv_exp.bn.bias
 (1024)" fillcolor=lightblue]
	139916968519696 -> 139916966763776
	139916966763776 [label=AccumulateGrad]
	139916966763968 -> 139916966764016
	139916966763968 [label=LeakyReluBackward1]
	139916966763200 -> 139916966763968
	139916966763200 [label=NativeBatchNormBackward0]
	139916966762624 -> 139916966763200
	139916966762624 [label=ConvolutionBackward0]
	139916966762192 -> 139916966762624
	139916966762192 [label=LeakyReluBackward0]
	139916966761424 -> 139916966762192
	139916966761424 [label=AddBackward0]
	139916966729472 -> 139916966761424
	139916966729472 [label=NativeBatchNormBackward0]
	139916966729712 -> 139916966729472
	139916966729712 [label=ConvolutionBackward0]
	139916966699456 -> 139916966729712
	139916966699456 [label=LeakyReluBackward1]
	139916966672128 -> 139916966699456
	139916966672128 [label=NativeBatchNormBackward0]
	139916966673616 -> 139916966672128
	139916966673616 [label=ConvolutionBackward0]
	139916966671696 -> 139916966673616
	139916966671696 [label=LeakyReluBackward1]
	139916966670976 -> 139916966671696
	139916966670976 [label=NativeBatchNormBackward0]
	139916966645568 -> 139916966670976
	139916966645568 [label=ConvolutionBackward0]
	139916966730624 -> 139916966645568
	139916966730624 [label=LeakyReluBackward0]
	139916966643840 -> 139916966730624
	139916966643840 [label=AddBackward0]
	139916966643312 -> 139916966643840
	139916966643312 [label=NativeBatchNormBackward0]
	139916966643264 -> 139916966643312
	139916966643264 [label=ConvolutionBackward0]
	139916966643072 -> 139916966643264
	139916966643072 [label=LeakyReluBackward1]
	139916966642640 -> 139916966643072
	139916966642640 [label=NativeBatchNormBackward0]
	139916966642448 -> 139916966642640
	139916966642448 [label=ConvolutionBackward0]
	139916966642256 -> 139916966642448
	139916966642256 [label=LeakyReluBackward1]
	139916966642112 -> 139916966642256
	139916966642112 [label=NativeBatchNormBackward0]
	139916966641920 -> 139916966642112
	139916966641920 [label=ConvolutionBackward0]
	139916966644080 -> 139916966641920
	139916966644080 [label=LeakyReluBackward0]
	139916967222912 -> 139916966644080
	139916967222912 [label=AddBackward0]
	139916967222768 -> 139916967222912
	139916967222768 [label=NativeBatchNormBackward0]
	139916967222480 -> 139916967222768
	139916967222480 [label=ConvolutionBackward0]
	139916967222096 -> 139916967222480
	139916967222096 [label=LeakyReluBackward1]
	139916967221952 -> 139916967222096
	139916967221952 [label=NativeBatchNormBackward0]
	139916967221808 -> 139916967221952
	139916967221808 [label=ConvolutionBackward0]
	139916967221568 -> 139916967221808
	139916967221568 [label=LeakyReluBackward1]
	139916967221136 -> 139916967221568
	139916967221136 [label=NativeBatchNormBackward0]
	139916967221040 -> 139916967221136
	139916967221040 [label=ConvolutionBackward0]
	139916967222864 -> 139916967221040
	139916967222864 [label=LeakyReluBackward0]
	139916967220560 -> 139916967222864
	139916967220560 [label=AddBackward0]
	139916967220368 -> 139916967220560
	139916967220368 [label=NativeBatchNormBackward0]
	139916967220224 -> 139916967220368
	139916967220224 [label=ConvolutionBackward0]
	139916967219888 -> 139916967220224
	139916967219888 [label=LeakyReluBackward1]
	139916967219600 -> 139916967219888
	139916967219600 [label=NativeBatchNormBackward0]
	139916967219408 -> 139916967219600
	139916967219408 [label=ConvolutionBackward0]
	139916967219264 -> 139916967219408
	139916967219264 [label=LeakyReluBackward1]
	139916967186096 -> 139916967219264
	139916967186096 [label=NativeBatchNormBackward0]
	139916967186048 -> 139916967186096
	139916967186048 [label=ConvolutionBackward0]
	139916967220608 -> 139916967186048
	139916967220608 [label=LeakyReluBackward0]
	139916967185472 -> 139916967220608
	139916967185472 [label=AddBackward0]
	139916967185280 -> 139916967185472
	139916967185280 [label=NativeBatchNormBackward0]
	139916967185136 -> 139916967185280
	139916967185136 [label=ConvolutionBackward0]
	139916967184896 -> 139916967185136
	139916967184896 [label=LeakyReluBackward1]
	139916967184464 -> 139916967184896
	139916967184464 [label=NativeBatchNormBackward0]
	139916967184368 -> 139916967184464
	139916967184368 [label=ConvolutionBackward0]
	139916967184080 -> 139916967184368
	139916967184080 [label=LeakyReluBackward1]
	139916967183936 -> 139916967184080
	139916967183936 [label=NativeBatchNormBackward0]
	139916967183744 -> 139916967183936
	139916967183744 [label=ConvolutionBackward0]
	139916966763728 -> 139916967183744
	139916967183360 -> 139916967183744
	139916968520176 [label="backbone.stages.2.blocks.0.conv1.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968520176 -> 139916967183360
	139916967183360 [label=AccumulateGrad]
	139916967183696 -> 139916967183936
	139916968520256 [label="backbone.stages.2.blocks.0.conv1.bn.weight
 (512)" fillcolor=lightblue]
	139916968520256 -> 139916967183696
	139916967183696 [label=AccumulateGrad]
	139916967184128 -> 139916967183936
	139916968520336 [label="backbone.stages.2.blocks.0.conv1.bn.bias
 (512)" fillcolor=lightblue]
	139916968520336 -> 139916967184128
	139916967184128 [label=AccumulateGrad]
	139916967184176 -> 139916967184368
	139916968520816 [label="backbone.stages.2.blocks.0.conv2.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916968520816 -> 139916967184176
	139916967184176 [label=AccumulateGrad]
	139916967184512 -> 139916967184464
	139916968520896 [label="backbone.stages.2.blocks.0.conv2.bn.weight
 (512)" fillcolor=lightblue]
	139916968520896 -> 139916967184512
	139916967184512 [label=AccumulateGrad]
	139916967184656 -> 139916967184464
	139916968520976 [label="backbone.stages.2.blocks.0.conv2.bn.bias
 (512)" fillcolor=lightblue]
	139916968520976 -> 139916967184656
	139916967184656 [label=AccumulateGrad]
	139916967184848 -> 139916967185136
	139916968521296 [label="backbone.stages.2.blocks.0.conv3.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968521296 -> 139916967184848
	139916967184848 [label=AccumulateGrad]
	139916967185232 -> 139916967185280
	139916968521376 [label="backbone.stages.2.blocks.0.conv3.bn.weight
 (512)" fillcolor=lightblue]
	139916968521376 -> 139916967185232
	139916967185232 [label=AccumulateGrad]
	139916967185856 -> 139916967185280
	139916968521456 [label="backbone.stages.2.blocks.0.conv3.bn.bias
 (512)" fillcolor=lightblue]
	139916968521456 -> 139916967185856
	139916967185856 [label=AccumulateGrad]
	139916966763728 -> 139916967185472
	139916967185664 -> 139916967186048
	139916968255712 [label="backbone.stages.2.blocks.1.conv1.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968255712 -> 139916967185664
	139916967185664 [label=AccumulateGrad]
	139916967186000 -> 139916967186096
	139916968255792 [label="backbone.stages.2.blocks.1.conv1.bn.weight
 (512)" fillcolor=lightblue]
	139916968255792 -> 139916967186000
	139916967186000 [label=AccumulateGrad]
	139916967186240 -> 139916967186096
	139916968255872 [label="backbone.stages.2.blocks.1.conv1.bn.bias
 (512)" fillcolor=lightblue]
	139916968255872 -> 139916967186240
	139916967186240 [label=AccumulateGrad]
	139916967186288 -> 139916967219408
	139916968256352 [label="backbone.stages.2.blocks.1.conv2.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916968256352 -> 139916967186288
	139916967186288 [label=AccumulateGrad]
	139916967219648 -> 139916967219600
	139916968256432 [label="backbone.stages.2.blocks.1.conv2.bn.weight
 (512)" fillcolor=lightblue]
	139916968256432 -> 139916967219648
	139916967219648 [label=AccumulateGrad]
	139916967219792 -> 139916967219600
	139916968256512 [label="backbone.stages.2.blocks.1.conv2.bn.bias
 (512)" fillcolor=lightblue]
	139916968256512 -> 139916967219792
	139916967219792 [label=AccumulateGrad]
	139916967219984 -> 139916967220224
	139916968256832 [label="backbone.stages.2.blocks.1.conv3.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968256832 -> 139916967219984
	139916967219984 [label=AccumulateGrad]
	139916967220176 -> 139916967220368
	139916968256912 [label="backbone.stages.2.blocks.1.conv3.bn.weight
 (512)" fillcolor=lightblue]
	139916968256912 -> 139916967220176
	139916967220176 [label=AccumulateGrad]
	139916967220416 -> 139916967220368
	139916968256992 [label="backbone.stages.2.blocks.1.conv3.bn.bias
 (512)" fillcolor=lightblue]
	139916968256992 -> 139916967220416
	139916967220416 [label=AccumulateGrad]
	139916967220608 -> 139916967220560
	139916967220752 -> 139916967221040
	139916968257392 [label="backbone.stages.2.blocks.2.conv1.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968257392 -> 139916967220752
	139916967220752 [label=AccumulateGrad]
	139916967221184 -> 139916967221136
	139916968257472 [label="backbone.stages.2.blocks.2.conv1.bn.weight
 (512)" fillcolor=lightblue]
	139916968257472 -> 139916967221184
	139916967221184 [label=AccumulateGrad]
	139916967221328 -> 139916967221136
	139916968257552 [label="backbone.stages.2.blocks.2.conv1.bn.bias
 (512)" fillcolor=lightblue]
	139916968257552 -> 139916967221328
	139916967221328 [label=AccumulateGrad]
	139916967221520 -> 139916967221808
	139916968258032 [label="backbone.stages.2.blocks.2.conv2.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916968258032 -> 139916967221520
	139916967221520 [label=AccumulateGrad]
	139916967221904 -> 139916967221952
	139916968258112 [label="backbone.stages.2.blocks.2.conv2.bn.weight
 (512)" fillcolor=lightblue]
	139916968258112 -> 139916967221904
	139916967221904 [label=AccumulateGrad]
	139916967222144 -> 139916967221952
	139916968258192 [label="backbone.stages.2.blocks.2.conv2.bn.bias
 (512)" fillcolor=lightblue]
	139916968258192 -> 139916967222144
	139916967222144 [label=AccumulateGrad]
	139916967222336 -> 139916967222480
	139916968258512 [label="backbone.stages.2.blocks.2.conv3.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968258512 -> 139916967222336
	139916967222336 [label=AccumulateGrad]
	139916967222720 -> 139916967222768
	139916968258592 [label="backbone.stages.2.blocks.2.conv3.bn.weight
 (512)" fillcolor=lightblue]
	139916968258592 -> 139916967222720
	139916967222720 [label=AccumulateGrad]
	139916967222672 -> 139916967222768
	139916968258672 [label="backbone.stages.2.blocks.2.conv3.bn.bias
 (512)" fillcolor=lightblue]
	139916968258672 -> 139916967222672
	139916967222672 [label=AccumulateGrad]
	139916967222864 -> 139916967222912
	139916967223104 -> 139916966641920
	139916968259072 [label="backbone.stages.2.blocks.3.conv1.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916968259072 -> 139916967223104
	139916967223104 [label=AccumulateGrad]
	139916966641872 -> 139916966642112
	139916968259152 [label="backbone.stages.2.blocks.3.conv1.bn.weight
 (512)" fillcolor=lightblue]
	139916968259152 -> 139916966641872
	139916966641872 [label=AccumulateGrad]
	139916966642160 -> 139916966642112
	139916968259232 [label="backbone.stages.2.blocks.3.conv1.bn.bias
 (512)" fillcolor=lightblue]
	139916968259232 -> 139916966642160
	139916966642160 [label=AccumulateGrad]
	139916966642304 -> 139916966642448
	139916967964896 [label="backbone.stages.2.blocks.3.conv2.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916967964896 -> 139916966642304
	139916966642304 [label=AccumulateGrad]
	139916966642688 -> 139916966642640
	139916967964976 [label="backbone.stages.2.blocks.3.conv2.bn.weight
 (512)" fillcolor=lightblue]
	139916967964976 -> 139916966642688
	139916966642688 [label=AccumulateGrad]
	139916966642832 -> 139916966642640
	139916967965056 [label="backbone.stages.2.blocks.3.conv2.bn.bias
 (512)" fillcolor=lightblue]
	139916967965056 -> 139916966642832
	139916966642832 [label=AccumulateGrad]
	139916966643024 -> 139916966643264
	139916967965376 [label="backbone.stages.2.blocks.3.conv3.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916967965376 -> 139916966643024
	139916966643024 [label=AccumulateGrad]
	139916966643600 -> 139916966643312
	139916967965456 [label="backbone.stages.2.blocks.3.conv3.bn.weight
 (512)" fillcolor=lightblue]
	139916967965456 -> 139916966643600
	139916966643600 [label=AccumulateGrad]
	139916966643456 -> 139916966643312
	139916967965536 [label="backbone.stages.2.blocks.3.conv3.bn.bias
 (512)" fillcolor=lightblue]
	139916967965536 -> 139916966643456
	139916966643456 [label=AccumulateGrad]
	139916966644080 -> 139916966643840
	139916966644752 -> 139916966645568
	139916967965936 [label="backbone.stages.2.blocks.4.conv1.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916967965936 -> 139916966644752
	139916966644752 [label=AccumulateGrad]
	139916966645184 -> 139916966670976
	139916967966016 [label="backbone.stages.2.blocks.4.conv1.bn.weight
 (512)" fillcolor=lightblue]
	139916967966016 -> 139916966645184
	139916966645184 [label=AccumulateGrad]
	139916966645520 -> 139916966670976
	139916967966096 [label="backbone.stages.2.blocks.4.conv1.bn.bias
 (512)" fillcolor=lightblue]
	139916967966096 -> 139916966645520
	139916966645520 [label=AccumulateGrad]
	139916966671312 -> 139916966673616
	139916967966576 [label="backbone.stages.2.blocks.4.conv2.conv.weight
 (512, 16, 3, 3)" fillcolor=lightblue]
	139916967966576 -> 139916966671312
	139916966671312 [label=AccumulateGrad]
	139916966673808 -> 139916966672128
	139916967966656 [label="backbone.stages.2.blocks.4.conv2.bn.weight
 (512)" fillcolor=lightblue]
	139916967966656 -> 139916966673808
	139916966673808 [label=AccumulateGrad]
	139916966673280 -> 139916966672128
	139916967966736 [label="backbone.stages.2.blocks.4.conv2.bn.bias
 (512)" fillcolor=lightblue]
	139916967966736 -> 139916966673280
	139916966673280 [label=AccumulateGrad]
	139916966702096 -> 139916966729712
	139916967967056 [label="backbone.stages.2.blocks.4.conv3.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916967967056 -> 139916966702096
	139916966702096 [label=AccumulateGrad]
	139916966702528 -> 139916966729472
	139916967967136 [label="backbone.stages.2.blocks.4.conv3.bn.weight
 (512)" fillcolor=lightblue]
	139916967967136 -> 139916966702528
	139916966702528 [label=AccumulateGrad]
	139916966702288 -> 139916966729472
	139916967967216 [label="backbone.stages.2.blocks.4.conv3.bn.bias
 (512)" fillcolor=lightblue]
	139916967967216 -> 139916966702288
	139916966702288 [label=AccumulateGrad]
	139916966730624 -> 139916966761424
	139916966761808 -> 139916966762624
	139916967967536 [label="backbone.stages.2.conv_transition_b.conv.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	139916967967536 -> 139916966761808
	139916966761808 [label=AccumulateGrad]
	139916966762384 -> 139916966763200
	139916967967616 [label="backbone.stages.2.conv_transition_b.bn.weight
 (512)" fillcolor=lightblue]
	139916967967616 -> 139916966762384
	139916966762384 [label=AccumulateGrad]
	139916966763536 -> 139916966763200
	139916967967696 [label="backbone.stages.2.conv_transition_b.bn.bias
 (512)" fillcolor=lightblue]
	139916967967696 -> 139916966763536
	139916966763536 [label=AccumulateGrad]
	139916966764112 -> 139916966764352
	139916967968096 [label="backbone.stages.2.conv_transition.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967968096 -> 139916966764112
	139916966764112 [label=AccumulateGrad]
	139916966764304 -> 139916966793280
	139916967968176 [label="backbone.stages.2.conv_transition.bn.weight
 (1024)" fillcolor=lightblue]
	139916967968176 -> 139916966764304
	139916966764304 [label=AccumulateGrad]
	139916966764496 -> 139916966793280
	139916967968256 [label="backbone.stages.2.conv_transition.bn.bias
 (1024)" fillcolor=lightblue]
	139916967968256 -> 139916966764496
	139916966764496 [label=AccumulateGrad]
	139916966793664 -> 139916966793808
	139916967665728 [label="backbone.stages.3.conv_down.conv.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	139916967665728 -> 139916966793664
	139916966793664 [label=AccumulateGrad]
	139916966793856 -> 139916966793904
	139916967665808 [label="backbone.stages.3.conv_down.bn.weight
 (1024)" fillcolor=lightblue]
	139916967665808 -> 139916966793856
	139916966793856 [label=AccumulateGrad]
	139916966794000 -> 139916966793904
	139916967665888 [label="backbone.stages.3.conv_down.bn.bias
 (1024)" fillcolor=lightblue]
	139916967665888 -> 139916966794000
	139916966794000 [label=AccumulateGrad]
	139916966794192 -> 139916966794624
	139916967666208 [label="backbone.stages.3.conv_exp.conv.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139916967666208 -> 139916966794192
	139916966794192 [label=AccumulateGrad]
	139916966794576 -> 139916966794672
	139916967666288 [label="backbone.stages.3.conv_exp.bn.weight
 (2048)" fillcolor=lightblue]
	139916967666288 -> 139916966794576
	139916966794576 [label=AccumulateGrad]
	139916966794816 -> 139916966794672
	139916967666368 [label="backbone.stages.3.conv_exp.bn.bias
 (2048)" fillcolor=lightblue]
	139916967666368 -> 139916966794816
	139916966794816 [label=AccumulateGrad]
	139916966795008 -> 139916966795200
	139916966795008 [label=LeakyReluBackward1]
	139916966731632 -> 139916966795008
	139916966731632 [label=NativeBatchNormBackward0]
	139916966793712 -> 139916966731632
	139916966793712 [label=ConvolutionBackward0]
	139916966793472 -> 139916966793712
	139916966793472 [label=LeakyReluBackward0]
	139916966763920 -> 139916966793472
	139916966763920 [label=AddBackward0]
	139916966761856 -> 139916966763920
	139916966761856 [label=NativeBatchNormBackward0]
	139916966700752 -> 139916966761856
	139916966700752 [label=ConvolutionBackward0]
	139916966670640 -> 139916966700752
	139916966670640 [label=LeakyReluBackward1]
	139916966643216 -> 139916966670640
	139916966643216 [label=NativeBatchNormBackward0]
	139916966643648 -> 139916966643216
	139916966643648 [label=ConvolutionBackward0]
	139916966642880 -> 139916966643648
	139916966642880 [label=LeakyReluBackward1]
	139916966642064 -> 139916966642880
	139916966642064 [label=NativeBatchNormBackward0]
	139916967223248 -> 139916966642064
	139916967223248 [label=ConvolutionBackward0]
	139916966763008 -> 139916967223248
	139916966763008 [label=LeakyReluBackward0]
	139916967220848 -> 139916966763008
	139916967220848 [label=AddBackward0]
	139916967221376 -> 139916967220848
	139916967221376 [label=NativeBatchNormBackward0]
	139916967220944 -> 139916967221376
	139916967220944 [label=ConvolutionBackward0]
	139916967219840 -> 139916967220944
	139916967219840 [label=LeakyReluBackward1]
	139916967185808 -> 139916967219840
	139916967185808 [label=NativeBatchNormBackward0]
	139916967185040 -> 139916967185808
	139916967185040 [label=ConvolutionBackward0]
	139916967184272 -> 139916967185040
	139916967184272 [label=LeakyReluBackward1]
	139916967183408 -> 139916967184272
	139916967183408 [label=NativeBatchNormBackward0]
	139916967183888 -> 139916967183408
	139916967183888 [label=ConvolutionBackward0]
	139916966794864 -> 139916967183888
	139916967183120 -> 139916967183888
	139916967666848 [label="backbone.stages.3.blocks.0.conv1.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967666848 -> 139916967183120
	139916967183120 [label=AccumulateGrad]
	139916967183504 -> 139916967183408
	139916967666928 [label="backbone.stages.3.blocks.0.conv1.bn.weight
 (1024)" fillcolor=lightblue]
	139916967666928 -> 139916967183504
	139916967183504 [label=AccumulateGrad]
	139916967184320 -> 139916967183408
	139916967667008 [label="backbone.stages.3.blocks.0.conv1.bn.bias
 (1024)" fillcolor=lightblue]
	139916967667008 -> 139916967184320
	139916967184320 [label=AccumulateGrad]
	139916967185088 -> 139916967185040
	139916967667488 [label="backbone.stages.3.blocks.0.conv2.conv.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	139916967667488 -> 139916967185088
	139916967185088 [label=AccumulateGrad]
	139916967186192 -> 139916967185808
	139916967667568 [label="backbone.stages.3.blocks.0.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	139916967667568 -> 139916967186192
	139916967186192 [label=AccumulateGrad]
	139916967186384 -> 139916967185808
	139916967667648 [label="backbone.stages.3.blocks.0.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	139916967667648 -> 139916967186384
	139916967186384 [label=AccumulateGrad]
	139916967219456 -> 139916967220944
	139916967667968 [label="backbone.stages.3.blocks.0.conv3.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967667968 -> 139916967219456
	139916967219456 [label=AccumulateGrad]
	139916967220080 -> 139916967221376
	139916967668048 [label="backbone.stages.3.blocks.0.conv3.bn.weight
 (1024)" fillcolor=lightblue]
	139916967668048 -> 139916967220080
	139916967220080 [label=AccumulateGrad]
	139916967222288 -> 139916967221376
	139916967668128 [label="backbone.stages.3.blocks.0.conv3.bn.bias
 (1024)" fillcolor=lightblue]
	139916967668128 -> 139916967222288
	139916967222288 [label=AccumulateGrad]
	139916966794864 -> 139916967220848
	139916967221712 -> 139916967223248
	139916967668528 [label="backbone.stages.3.blocks.1.conv1.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967668528 -> 139916967221712
	139916967221712 [label=AccumulateGrad]
	139916967222528 -> 139916966642064
	139916967668608 [label="backbone.stages.3.blocks.1.conv1.bn.weight
 (1024)" fillcolor=lightblue]
	139916967668608 -> 139916967222528
	139916967222528 [label=AccumulateGrad]
	139916967223056 -> 139916966642064
	139916967668688 [label="backbone.stages.3.blocks.1.conv1.bn.bias
 (1024)" fillcolor=lightblue]
	139916967668688 -> 139916967223056
	139916967223056 [label=AccumulateGrad]
	139916966642496 -> 139916966643648
	139916967669168 [label="backbone.stages.3.blocks.1.conv2.conv.weight
 (1024, 32, 3, 3)" fillcolor=lightblue]
	139916967669168 -> 139916966642496
	139916966642496 [label=AccumulateGrad]
	139916966644416 -> 139916966643216
	139916967669248 [label="backbone.stages.3.blocks.1.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	139916967669248 -> 139916966644416
	139916966644416 [label=AccumulateGrad]
	139916966644368 -> 139916966643216
	139916967669328 [label="backbone.stages.3.blocks.1.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	139916967669328 -> 139916966644368
	139916966644368 [label=AccumulateGrad]
	139916966674192 -> 139916966700752
	139916967669648 [label="backbone.stages.3.blocks.1.conv3.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967669648 -> 139916966674192
	139916966674192 [label=AccumulateGrad]
	139916966700080 -> 139916966761856
	139916967374912 [label="backbone.stages.3.blocks.1.conv3.bn.weight
 (1024)" fillcolor=lightblue]
	139916967374912 -> 139916966700080
	139916966700080 [label=AccumulateGrad]
	139916966760848 -> 139916966761856
	139916967374992 [label="backbone.stages.3.blocks.1.conv3.bn.bias
 (1024)" fillcolor=lightblue]
	139916967374992 -> 139916966760848
	139916966760848 [label=AccumulateGrad]
	139916966763008 -> 139916966763920
	139916966793616 -> 139916966793712
	139916967375312 [label="backbone.stages.3.conv_transition_b.conv.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	139916967375312 -> 139916966793616
	139916966793616 [label=AccumulateGrad]
	139916966794432 -> 139916966731632
	139916967375392 [label="backbone.stages.3.conv_transition_b.bn.weight
 (1024)" fillcolor=lightblue]
	139916967375392 -> 139916966794432
	139916966794432 [label=AccumulateGrad]
	139916966794768 -> 139916966731632
	139916967375472 [label="backbone.stages.3.conv_transition_b.bn.bias
 (1024)" fillcolor=lightblue]
	139916967375472 -> 139916966794768
	139916966794768 [label=AccumulateGrad]
	139916966795152 -> 139916966795584
	139916967375872 [label="backbone.stages.3.conv_transition.conv.weight
 (2048, 2048, 1, 1)" fillcolor=lightblue]
	139916967375872 -> 139916966795152
	139916966795152 [label=AccumulateGrad]
	139916966795536 -> 139916966795632
	139916967375952 [label="backbone.stages.3.conv_transition.bn.weight
 (2048)" fillcolor=lightblue]
	139916967375952 -> 139916966795536
	139916966795536 [label=AccumulateGrad]
	139916966795968 -> 139916966795632
	139916967376032 [label="backbone.stages.3.conv_transition.bn.bias
 (2048)" fillcolor=lightblue]
	139916967376032 -> 139916966795968
	139916966795968 [label=AccumulateGrad]
	139916966796304 -> 139916966796112
	139916966796304 [label=TBackward0]
	139916966795728 -> 139916966796304
	139916967378832 [label="fc_class_count.weight
 (23, 2048)" fillcolor=lightblue]
	139916967378832 -> 139916966795728
	139916966795728 [label=AccumulateGrad]
	139916966796112 -> 139917091875232
	139916966742432 [label="
 (1, 9)" fillcolor=darkolivegreen1]
	139916966796736 [label=AddmmBackward0]
	139916966795344 -> 139916966796736
	139916967378592 [label="fc_time.bias
 (9)" fillcolor=lightblue]
	139916967378592 -> 139916966795344
	139916966795344 [label=AccumulateGrad]
	139916966796784 -> 139916966796736
	139916966795392 -> 139916966796736
	139916966795392 [label=TBackward0]
	139916966796688 -> 139916966795392
	139916967378672 [label="fc_time.weight
 (9, 2048)" fillcolor=lightblue]
	139916967378672 -> 139916966796688
	139916966796688 [label=AccumulateGrad]
	139916966796736 -> 139916966742432
}
